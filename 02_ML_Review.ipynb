{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What:\n",
    "- Cross Validation allows us to compare different Machine Learning methods and get a sense of how well they will perform in practice.\n",
    "- In the real world, you DO NOT re-use the same data for both training and testing. You create a train-test split. However, how you create the train-test split can have different effects. \n",
    "- One work around for this, is 10-fold Cross Validation. \n",
    "- You also use Cross Validation for hyper-parameter tuning. Where you train the model across different folds with various hyper-parameter configurations, you then pick the hyper-parameter that yeilded the best results.\n",
    "\n",
    "\n",
    "How:\n",
    "- For 10-fold cross validation, you create 10 splits in the dataset, train on 9 blocks, and test the model on 1. You repeat this across all different blocks/folds in the dataset. \n",
    "- You summarize the results from each block/split. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                        | Actual Positive          | Actual Negative          |\n",
    "|------------------------|--------------------------|--------------------------|\n",
    "| Predicted Positive     | True Positive (TP)       | False Positive (FP)      |\n",
    "| Predicted Negative     | False Negative (FN)      | True Negative (TN)       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Sensitivity (Recall) = (\\frac{TP}{TP + FN})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity defines what number of items with treatment (TP) were correctly identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$$ Specificity = (\\frac{TN}{TN + FP}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity defines what number of items without treatment (TN) were correctly identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Precision = (\\frac{TP}{TP + FP})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the proportion of positive results that were correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Bias is the difference between the prediction of our model and the correct value.\n",
    "- Model with high bias pays very little attention to the training data, and oversimplifies the model. It leads to high error on the training and test dataset.\n",
    "- Ex. Fitting a straight line on a curved training dataset, leads to high-bias. As the model fits the curved training data poorly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variance is the variability of model prediction for a given data point or value which tells us spread of the data.\n",
    "- Model with high variance pays a lot of attention to training data, and does not generalize on the data which it hasn't seen before. \n",
    "Ex. Overfitting the highly curved line on the specific training dataset. The model overfits on the training dataset, and performs poorly on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                        | Low Variance         | High Variance          |\n",
    "|------------------------|--------------------------|--------------------------|\n",
    "| High Bias     | Underfitting       | Horrible      |\n",
    "| Low Bias     | Perfect      | Overfitting       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the model is too simple and has very few parameters, then it may have high bias and low variance. \n",
    "- On the other hand, if the model has large number of parameters, then it could have high variance and low bias. \n",
    "\n",
    "- In ML, a good ML model has low bias and low variance, by producing consistent results across different datasets.\n",
    "- This is done, by finding the sweet spot between simple models and complex models. This done usually with the help of regularization, boosting, and bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC (Receiver Operating Characteristic) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A curve used to evaluate the performance of a binary classification model. \n",
    "- ROC curve plots:\n",
    "    1. True Positive Rate (TPR) on the Y-axis, also called sensitivity or recall. \n",
    "    2. False Positive Rate (FPR) on the X-axis, calculated as (1 - specififity).\n",
    "\n",
    "    Each point on the curve represents a different 'threshold' for deciding whether a predicated probability counts as a positive or negative classification. \n",
    "\n",
    "\n",
    "What the curve tells you?\n",
    "- Ideal: Passes through the top-left corner (TPR=1, FPR=0)\n",
    "- Random model: Forms a diagonal line from (0, 0) to (1,1).\n",
    "- Area Under Curve (AUC): A number between 0 and 1- higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive Entropy, we first need to understand 'Surprise'. How suprised are you when an outcome appears that is extremely less likely? \n",
    "\n",
    "- Surprise is the inverse of the probability of an event occurring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$$ Surprise = (log(\\frac{1}{Probability}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entroy is the expected value of the surprise. In other words, you multiply Probability with surprise for all events, to calculate Entropy or expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Entropy(Surprise) = (ProbabilityOfEvent)(log(\\frac{1}{Probability_Of_Event})) +\n",
    "(ProbabilityOfNOEvent)(log(\\frac{1}{ProbabilityOfNOEvent}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- R-squared is defined as:\n",
    "$$R^2 = \\frac{Var(Mean) - Var(Fit)}{Var(Mean)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 helps one understand how much of the variance in mean of a given feature is explained by another feature. For example, how much of the variance in mean mouse weight, is explained by the variance by mean mouse size regressed weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Ordinary Least Squares (OLS) to fit a line to the data.\n",
    "- Calculate R^2\n",
    "- Calculate p-value for R^2\n",
    "\n",
    "Equation of Line:\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation of a line in matrix representation.\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "where, \n",
    "- y is an nx1 vector of observed values.\n",
    "- X is an n x (p+1) matrix (including a column of ones for the intercept).\n",
    "- Beta/B is a (p+1) x 1 vector of coefficients.\n",
    "- Epsilon is the residual error of vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of OLS:\n",
    "- The goal is to choose $\\beta$ such that the sum of squared residuals is minimized. \n",
    "\n",
    "$$\n",
    "S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of OLS Estimator:\n",
    "\n",
    "Step 1: Express the cost function\n",
    "\n",
    "$$\n",
    "S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "Step 2: Take the derivative with respect to $( \\boldsymbol{\\beta} )$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "Step 3: Set the derivative equal to 0 and solve\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Step 4: Solve for the OLS estimator\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "- **Estimated coefficients**:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "- **Predicted values**:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\n",
    "$$\n",
    "\n",
    "- **Residuals**:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{y} - \\hat{\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of OLS\n",
    "\n",
    "1. **Linearity**: The model is linear in parameters  \n",
    "2. **Full rank**: $( \\mathbf{X}^T \\mathbf{X} )$ is invertible  \n",
    "3. **Exogeneity**: $( \\mathbb{E}[\\boldsymbol{\\varepsilon} | \\mathbf{X}] = 0 )$  \n",
    "4. **Homoscedasticity**: Constant variance of errors  \n",
    "5. **No autocorrelation**: Errors are uncorrelated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
